{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c548a60c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:46.808619Z",
     "iopub.status.busy": "2024-10-20T00:47:46.808196Z",
     "iopub.status.idle": "2024-10-20T00:47:50.806345Z",
     "shell.execute_reply": "2024-10-20T00:47:50.805324Z"
    },
    "papermill": {
     "duration": 4.01265,
     "end_time": "2024-10-20T00:47:50.809175",
     "exception": false,
     "start_time": "2024-10-20T00:47:46.796525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import  CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b2ed92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:50.832620Z",
     "iopub.status.busy": "2024-10-20T00:47:50.831467Z",
     "iopub.status.idle": "2024-10-20T00:47:50.837672Z",
     "shell.execute_reply": "2024-10-20T00:47:50.836562Z"
    },
    "papermill": {
     "duration": 0.020158,
     "end_time": "2024-10-20T00:47:50.840043",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.819885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT            = Path(\"../data\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "TARGET = 'target'\n",
    "EVAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fc7d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:50.863081Z",
     "iopub.status.busy": "2024-10-20T00:47:50.862172Z",
     "iopub.status.idle": "2024-10-20T00:47:50.898206Z",
     "shell.execute_reply": "2024-10-20T00:47:50.897114Z"
    },
    "papermill": {
     "duration": 0.050623,
     "end_time": "2024-10-20T00:47:50.900917",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.850294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TreeModelling:\n",
    "    \"\"\"\n",
    "        Train and test data should contain the same selected features for ML models.\n",
    "        Train, test data and target should be the same data type. (Pandas or Numpy)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config \n",
    "    \n",
    "    def lgb_train(self, x_tr, y_tr, params):\n",
    "        \n",
    "        model = lgb.train(params,\n",
    "                          lgb.Dataset(x_tr, y_tr))\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def lgb_train_and_valid(self, x_tr, y_tr, x_val, y_val, params):\n",
    "            \n",
    "        dtrain = lgb.Dataset(x_tr, y_tr)\n",
    "        deval = lgb.Dataset(x_val, y_val, reference=dtrain)\n",
    "\n",
    "        ####\n",
    "        dtrain.week_num = train_week\n",
    "        deval.week_num =  valid_week\n",
    "        ####\n",
    "\n",
    "        callbacks = [\n",
    "                lgb.early_stopping(stopping_rounds=self.config[\"lgb\"][\"es_round\"],\n",
    "                                   first_metric_only = True), \n",
    "                lgb.log_evaluation(self.config[\"lgb\"][\"verbose_eval\"])\n",
    "            ]\n",
    "            \n",
    "        model = lgb.train(params, dtrain, \n",
    "                          self.config[\"lgb\"][\"num_round\"], \n",
    "                          valid_sets=[deval], \n",
    "                          callbacks = callbacks)\n",
    "        valid_pred = model.predict(x_val)\n",
    "    \n",
    "        return model, valid_pred\n",
    "        \n",
    "    def lgb_test(self, models, test):\n",
    "        test_pred = [model.predict(test) for model in models]\n",
    "        test_pred = np.mean(test_pred, axis=0)\n",
    "        return test_pred\n",
    "    \n",
    "    def lgb_test_by_batch(self, models, test, batch_size):\n",
    "        test_pred_all = []\n",
    "        for idx in range(0, len(test), batch_size):\n",
    "            test_pred_batch = [model.predict(test.iloc[idx:idx+batch_size]) for model in models]\n",
    "            test_pred_batch = np.mean(test_pred_batch, axis=0)\n",
    "            test_pred_all.append(test_pred_batch)\n",
    "        return np.concatenate(test_pred_all)\n",
    "    \n",
    "    def lgb_numpy_test_by_batch(self, models, test, batch_size):\n",
    "        test_pred_all = []\n",
    "        for idx in range(0, len(test), batch_size):\n",
    "            test_pred_batch = [model.predict(test[idx:idx+batch_size]) for model in models]\n",
    "            test_pred_batch = np.mean(test_pred_batch, axis=0)\n",
    "            test_pred_all.append(test_pred_batch)\n",
    "        return np.concatenate(test_pred_all)\n",
    "    \n",
    "\n",
    "    def xgb_train_and_valid(self, x_tr, y_tr, x_val, y_val, params):\n",
    "                \n",
    "        xgb_eval = xgb.DMatrix(x_val, label = y_val)\n",
    "        model = xgb.train(params,\n",
    "                          xgb.DMatrix(x_tr, label = y_tr), \n",
    "                          self.config[\"xgb\"][\"num_round\"],\n",
    "                          evals = [(xgb_eval, \"eval\")], \n",
    "                          early_stopping_rounds = self.config[\"xgb\"][\"es_round\"], \n",
    "                          verbose_eval = self.config[\"xgb\"][\"verbose_eval\"])\n",
    "        valid_pred = model.predict(xgb_eval, iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "        return model, valid_pred\n",
    "    \n",
    "    def xgb_test(self, models, test):\n",
    "        dtest = xgb.DMatrix(test)\n",
    "        test_pred = [model.predict(dtest, iteration_range=(0, model.best_ntree_limit)) for model in models]\n",
    "        test_pred = np.mean(test_pred, axis=0)\n",
    "        return test_pred\n",
    "    \n",
    "\n",
    "    def cb_train(self, x_tr, y_tr, params, cat):\n",
    "        \n",
    "        train_pool = Pool(data=x_tr,\n",
    "                          label=y_tr,\n",
    "                          cat_features=cat\n",
    "                         )\n",
    "                \n",
    "        if self.config[\"cb\"][\"task_type\"] == \"classification\":\n",
    "            model = CatBoostClassifier(**params)\n",
    "\n",
    "        elif self.config[\"cb\"][\"task_type\"]:\n",
    "            model = CatBoostRegressor(**params)\n",
    "        model.fit(train_pool)\n",
    "            \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def cb_train_and_valid(self,  x_tr, y_tr, x_val, y_val, params, cat):\n",
    "        \n",
    "        train_pool = Pool(data=x_tr,\n",
    "                          label=y_tr,\n",
    "                          cat_features=cat)\n",
    "        \n",
    "        valid_pool = Pool(data=x_val,\n",
    "                          label=y_val,\n",
    "                          cat_features=cat)\n",
    "                \n",
    "        if self.config[\"cb\"][\"task_type\"] == \"classification\":\n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(train_pool,\n",
    "                      eval_set=[valid_pool], \n",
    "                      early_stopping_rounds=self.config[\"cb\"][\"es_round\"],\n",
    "                      verbose_eval = self.config[\"cb\"][\"verbose_eval\"])\n",
    "            valid_pred = model.predict_proba(x_val)[:,1]\n",
    "        elif self.config[\"cb\"][\"task_type\"]:\n",
    "            model = CatBoostRegressor(**params)\n",
    "            model.fit(train_pool,\n",
    "                      eval_set=[valid_pool], \n",
    "                      early_stopping_rounds=self.config[\"cb\"][\"es_round\"], \n",
    "                      verbose_eval = self.config[\"cb\"][\"verbose_eval\"])\n",
    "            valid_pred = model.predict(x_val)\n",
    "            \n",
    "        return model, valid_pred\n",
    "    \n",
    "    def cb_test(self, models, test):\n",
    "        if self.config[\"cb\"][\"task_type\"] == \"classification\":\n",
    "            test_pred = [model.predict_proba(test)[:,1] for model in models]\n",
    "        else:\n",
    "            test_pred = [model.predict(test) for model in models]\n",
    "        test_pred = np.mean(test_pred, axis=0)        \n",
    "        return test_pred\n",
    "    \n",
    "    def cb_test_by_batch(self, models, test, batch_size):\n",
    "        test_pred_all = []\n",
    "        for idx in range(0, len(test), batch_size):\n",
    "            if self.config[\"cb\"][\"task_type\"] == \"classification\":\n",
    "                test_pred_batch = [model.predict_proba(test.iloc[idx:idx+batch_size])[:,1] for model in models]\n",
    "            else:\n",
    "                test_pred_batch = [model.predict(test.iloc[idx:idx+batch_size]) for model in models]            \n",
    "            test_pred_batch = np.mean(test_pred_batch, axis=0)\n",
    "            test_pred_all.append(test_pred_batch)\n",
    "        return np.concatenate(test_pred_all)\n",
    "    \n",
    "    def cb_numpy_test_by_batch(self, models, test, batch_size):\n",
    "        test_pred_all = []\n",
    "        for idx in range(0, len(test), batch_size):\n",
    "            if self.config[\"cb\"][\"task_type\"] == \"classification\":\n",
    "                test_pred_batch = [model.predict_proba(test[idx:idx+batch_size])[:,1] for model in models]\n",
    "            else:\n",
    "                test_pred_batch = [model.predict(test[idx:idx+batch_size]) for model in models]            \n",
    "            test_pred_batch = np.mean(test_pred_batch, axis=0)\n",
    "            test_pred_all.append(test_pred_batch)\n",
    "        return np.concatenate(test_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab41c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:50.924670Z",
     "iopub.status.busy": "2024-10-20T00:47:50.924252Z",
     "iopub.status.idle": "2024-10-20T00:47:50.939832Z",
     "shell.execute_reply": "2024-10-20T00:47:50.938727Z"
    },
    "papermill": {
     "duration": 0.030781,
     "end_time": "2024-10-20T00:47:50.942187",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.911406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d492b",
   "metadata": {
    "papermill": {
     "duration": 0.010208,
     "end_time": "2024-10-20T00:47:50.962632",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.952424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "------- edit from here -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1d027",
   "metadata": {
    "papermill": {
     "duration": 0.010105,
     "end_time": "2024-10-20T00:47:50.982970",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.972865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# common fe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a42010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:51.006719Z",
     "iopub.status.busy": "2024-10-20T00:47:51.005929Z",
     "iopub.status.idle": "2024-10-20T00:47:51.021891Z",
     "shell.execute_reply": "2024-10-20T00:47:51.020727Z"
    },
    "papermill": {
     "duration": 0.030948,
     "end_time": "2024-10-20T00:47:51.024210",
     "exception": false,
     "start_time": "2024-10-20T00:47:50.993262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df): #Standardize the dtype.\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df): #Change the feature for D to the difference in days from date_decision.\n",
    "        for col in df.columns:\n",
    "            if (col[-1] in (\"D\",)) and ('count' not in col):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df): #Remove those with an average is_null exceeding 0.95 and those that do not fall within the range 1 < nunique < 200.\n",
    "        drop_cols = []\n",
    "        for col in df.columns:\n",
    "            #if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "            #    isnull = df[col].is_null().mean()\n",
    "            #    if isnull > 0.95:\n",
    "            #        drop_cols.append(col)\n",
    "\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\", ]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 10):#50 #len(df) * 0.20): # 95 # fe4 down at fq20\n",
    "                    drop_cols.append(col)\n",
    "            \n",
    "            # eliminate yaer, month feature\n",
    "            # 644\n",
    "            if (col[-1] not in [\"P\", \"A\", \"L\", \"M\"]) and (('month_' in col) or ('year_' in col)):# or ('num_group' in col):\n",
    "            # if (('month_' in col) or ('year_' in col)):# or ('num_group' in col):\n",
    "                drop_cols.append(col)\n",
    "\n",
    "        return drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15dbe9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:51.047666Z",
     "iopub.status.busy": "2024-10-20T00:47:51.047198Z",
     "iopub.status.idle": "2024-10-20T00:47:51.110433Z",
     "shell.execute_reply": "2024-10-20T00:47:51.109304Z"
    },
    "papermill": {
     "duration": 0.078519,
     "end_time": "2024-10-20T00:47:51.113172",
     "exception": false,
     "start_time": "2024-10-20T00:47:51.034653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        # expr_3 = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "        # expr_3 = [pl.var(col).alias(f\"var_{col}\") for col in cols]+ [pl.sum(col).alias(f\"sum_{col}\") for col in cols]\n",
    "        # expr_3 = [pl.last(col).alias(f\"last_{col}\") for col in cols] #+ \\\n",
    "        #     [pl.first(col).alias(f\"first_{col}\") for col in cols] + \\\n",
    "        #     [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + \\\n",
    "        #     [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "        # expr_3 = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "\n",
    "        cols2 = [col for col in df.columns if col[-1] in (\"L\", \"A\")]\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + \\\n",
    "                 [pl.std(col).alias(f\"std_{col}\") for col in cols2] + \\\n",
    "                 [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + \\\n",
    "                 [pl.median(col).alias(f\"median_{col}\") for col in cols2] # + \\\n",
    "            # [pl.first(col).alias(f\"first_{col}\") for col in cols2] + [pl.last(col).alias(f\"last_{col}\") for col in cols2]\n",
    "        \n",
    "        # BAD\n",
    "        # cols3 = [col for col in df.columns if col[-1] in (\"A\")]\n",
    "        # expr_4 = [pl.col(col).fill_null(strategy=\"zero\").apply(lambda x: x.max() - x.min()).alias(f\"max-min_gap_{col}\") \n",
    "        #           for col in cols3]\n",
    "        return expr_1 + expr_2 + expr_3 # + [pl.col(col).diff().last().alias(f\"diff-last_{col}\") for col in cols3] # + expr_4\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def bureau_a1(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "\n",
    "        cols2 = [\n",
    "            # bad\n",
    "        'annualeffectiverate_199L', 'annualeffectiverate_63L',\n",
    "        'contractsum_5085717L', \n",
    "        'credlmt_230A', 'credlmt_935A',\n",
    "        # 'debtoutstand_525A', 'debtoverdue_47A', 'dpdmax_139P', 'dpdmax_757P',\n",
    "    #    'instlamount_768A', 'instlamount_852A',\n",
    "    #    'interestrate_508L', 'monthlyinstlamount_332A',\n",
    "    #    'monthlyinstlamount_674A', \n",
    "            # good?\n",
    "       'nominalrate_281L', 'nominalrate_498L',\n",
    "       'numberofcontrsvalue_258L', 'numberofcontrsvalue_358L',\n",
    "       'numberofinstls_229L', 'numberofinstls_320L',\n",
    "       'numberofoutstandinstls_520L', 'numberofoutstandinstls_59L',\n",
    "       'numberofoverdueinstlmax_1039L', 'numberofoverdueinstlmax_1151L',\n",
    "       'numberofoverdueinstls_725L', 'numberofoverdueinstls_834L',\n",
    "            # bad?\n",
    "    #    'outstandingamount_354A', 'outstandingamount_362A', 'overdueamount_31A',\n",
    "    #    'overdueamount_659A', 'overdueamountmax2_14A', 'overdueamountmax2_398A',\n",
    "    #    'overdueamountmax_155A', 'overdueamountmax_35A',\n",
    "        # bad ?\n",
    "    #    'periodicityofpmts_1102L', 'periodicityofpmts_837L',\n",
    "    #    'prolongationcount_1120L', 'prolongationcount_599L',\n",
    "        # 520?\n",
    "    #    'residualamount_488A', 'residualamount_856A', 'totalamount_6A',\n",
    "    #    'totalamount_996A', 'totaldebtoverduevalue_178A',\n",
    "    #    'totaldebtoverduevalue_718A', 'totaloutstanddebtvalue_39A',\n",
    "    #    'totaloutstanddebtvalue_668A',\n",
    "       ]\n",
    "\n",
    "        # .697\n",
    "        # expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + [pl.std(col).alias(f\"std_{col}\") for col in cols2]\n",
    "        \n",
    "        # .696\n",
    "        # expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2]\n",
    "\n",
    "        # .697\n",
    "        # expr_3 = [pl.std(col).alias(f\"std_{col}\") for col in cols2]\n",
    "        \n",
    "        # .6985\n",
    "        # expr_3 = [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2]\n",
    "\n",
    "        # .696\n",
    "        # expr_3 = [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] \n",
    "\n",
    "        # .6981\n",
    "        # expr_3 = [pl.median(col).alias(f\"median_{col}\") for col in cols2]\n",
    "\n",
    "        # .696\n",
    "        # expr_3 = [pl.first(col).alias(f\"first_{col}\") for col in cols2] + [pl.last(col).alias(f\"last_{col}\") for col in cols2] # + \\\n",
    "        \n",
    "        # .696\n",
    "        # expr_3 = [pl.std(col).alias(f\"std_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2]\n",
    "\n",
    "        # .699\n",
    "        # expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + [pl.std(col).alias(f\"std_{col}\") for col in cols2] + \\\n",
    "        #     [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2]\n",
    "\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_{col}\") for col in cols2] + [pl.std(col).alias(f\"std_{col}\") for col in cols2] + \\\n",
    "            [pl.sum(col).alias(f\"sum_{col}\") for col in cols2] + [pl.median(col).alias(f\"median_{col}\") for col in cols2] + \\\n",
    "            [pl.first(col).alias(f\"first_{col}\") for col in cols2] # + [pl.last(col).alias(f\"last_{col}\") for col in cols2] # not applied\n",
    "        \n",
    "        \n",
    "\n",
    "        # expr_3 = [pl.col(col).fill_null(strategy=\"zero\").apply(lambda x: x.max() - x.min()).alias(f\"max-min_gap_depth2_{col}\") for col in cols2]\n",
    "        return expr_1 + expr_2 + expr_3    \n",
    "\n",
    "    @staticmethod\n",
    "    def deposit_exprs(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] # + \\\n",
    "            # [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "            # [pl.mean(col).alias(f\"mean_{col}\") for col in cols] # + \\\n",
    "            # [pl.std(col).alias(f\"std_{col}\") for col in cols]  + \\\n",
    "             \n",
    "            # [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_2 = [pl.first('openingdate_857D').alias(f'first_openingdate_857D')] + [pl.last('openingdate_857D').alias(f'last_openingdate_857D')]\n",
    "        \n",
    "        return expr_1 # + expr_2 #+ expr_ngmax\n",
    "\n",
    "    @staticmethod\n",
    "    def debitcard_exprs(df):\n",
    "        # cols = [col for col in df.columns if (col[-1] in [\"A\"])]\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] \n",
    "            # [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + \\\n",
    "            # [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "        # expr_2 = [pl.first('openingdate_857D').alias(f'first_openingdate_857D')] + [pl.last('openingdate_857D').alias(f'last_openingdate_857D')]\n",
    "        \n",
    "        return expr_1 # + expr_2 #+ expr_ngmax\n",
    "        # return expr_1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def person_expr(df):\n",
    "        cols1 = ['empl_employedtotal_800L', 'empl_employedfrom_271D', 'empl_industry_691L', \n",
    "                 'familystate_447L', 'incometype_1044T', 'sex_738L', 'housetype_905L', 'housingtype_772L',\n",
    "                 'isreference_387L', 'birth_259D', ]\n",
    "        # cols1 = [col for col in df.columns]\n",
    "        expr_1 = [pl.first(col).alias(f\"first_{col}\") for col in cols1]\n",
    "        \n",
    "        expr_2 = [pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"), \n",
    "                  pl.col(\"mainoccupationinc_384A\").filter(pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")]\n",
    "        \n",
    "        # No Effect ...\n",
    "        # cols = ['personindex_1023L', 'persontype_1072L', 'persontype_792L']\n",
    "        # expr_3 = [pl.col(col).last().alias(f\"last_{col}\") for col in cols] + [pl.col(col).drop_nulls().mean().alias(f\"mean_{col}\") for col in cols]\n",
    "\n",
    "        # cols2 = [col for col in df.columns if col not in cols1]\n",
    "        # expr_4 = [pl.max(col).alias(f\"max_{col}\") for col in cols2] + [pl.min(col).alias(f\"min_{col}\") for col in cols2] #  good at cv, bad at lb ?\n",
    "            # [pl.col(col).drop_nulls().last().alias(f\"last_{col}\") for col in cols2] + [pl.col(col).drop_nulls().first().alias(f\"first_{col}\") for col in cols2] # no effect\n",
    "\n",
    "        return expr_1 + expr_2 # + expr_4 # + expr_3\n",
    "    \n",
    "    @staticmethod\n",
    "    def person_2_expr(df):\n",
    "        # cols = [col for col in df.columns]\n",
    "        cols = ['empls_economicalst_849M', 'empls_employedfrom_796D', 'empls_employer_name_740M'] # + \\\n",
    "            # ['relatedpersons_role_762T', 'conts_role_79M']\n",
    "            # ['addres_district_368M', 'addres_role_871L', 'addres_zip_823M']\n",
    "\n",
    "        expr_1 = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_2 = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "\n",
    "        # BAD\n",
    "        # expr_ngc = [pl.count(\"num_group2\").alias(f\"count_num_group2\")]\n",
    "        # cols2 = [col for col in df.columns if (col in (\"num_group1\", \"num_group2\"))]\n",
    "        # expr_ngmax = [pl.min(col).alias(f\"min_{col}\") for col in cols2] + [pl.max(col).alias(f\"max_{col}\") for col in cols2]\n",
    "\n",
    "        # cols2 = [col for col in df.columns if col not in cols]\n",
    "        # # expr_3 = [pl.max(col).alias(f\"max_{col}\") for col in cols2] + [pl.min(col).alias(f\"min_{col}\") for col in cols2] # no effect\n",
    "        # expr_3 = [pl.col(col).drop_nulls().last().alias(f\"last_{col}\") for col in cols2] # no effect\n",
    "\n",
    "        return expr_1 + expr_2 # + expr_3# + expr_ngc \n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        expr_1 = [pl.first(col).alias(f\"__other_{col}\") for col in df.columns if ('num_group' not in col) and (col != 'case_id')]\n",
    "        # cols1 = ['amtdepositbalance_4809441A', 'amtdepositincoming_4809444A', 'amtdepositoutgoing_4809442A']\n",
    "        # expr_1 = [pl.last(col).alias(f\"last_{col}\") for col in cols1]\n",
    "        # cols2 = ['amtdebitincoming_4809443A', 'amtdebitoutgoing_4809440A']\n",
    "        # expr_3 = [(pl.col('amtdebitincoming_4809443A') - pl.col('amtdebitoutgoing_4809440A')).alias('amtdebit_incoming-outgoing')]\n",
    "        return expr_1 # + expr_2 + expr_3\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def tax_a_exprs(df):\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "        expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] + \\\n",
    "            [pl.last(col).alias(f\"last_{col}\") for col in cols] + \\\n",
    "            [pl.first(col).alias(f\"first_{col}\") for col in cols] + \\\n",
    "            [pl.mean(col).alias(f\"mean_{col}\") for col in cols] + \\\n",
    "            [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "        # expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in ['amount_4527230A', 'recorddate_4527225D', 'num_group1']] + \\\n",
    "        #     [pl.min(col).alias(f\"min_{col}\") for col in ['amount_4527230A', 'recorddate_4527225D', ]] + \\\n",
    "        #     [pl.mean(col).alias(f\"mean_{col}\") for col in ['amount_4527230A']] + \\\n",
    "        #     [pl.std(col).alias(f\"std_{col}\") for col in ['amount_4527230A']] + \\\n",
    "        #     [pl.last(col).alias(f\"last_{col}\") for col in ['amount_4527230A', 'recorddate_4527225D', 'name_4527232M']] + \\\n",
    "        #     [pl.first(col).alias(f\"first_{col}\") for col in ['amount_4527230A', 'recorddate_4527225D', 'name_4527232M']] # BAD?\n",
    "\n",
    "        expr_4 = [pl.col(col).fill_null(strategy=\"zero\").map_elements(lambda x: x.max() - x.min(), return_dtype=pl.Float32).alias(f\"max-min_gap_depth2_{col}\") for col in ['amount_4527230A']]\n",
    "\n",
    "        return expr_1 + expr_4\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bureau_a2(df): # 122만\n",
    "        # cols = ['collater_valueofguarantee_1124L', 'pmts_dpd_1073P', 'pmts_overdue_1140A',]\n",
    "        cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        expr_1 = [pl.max(col).alias(f\"max_depth2_{col}\") for col in cols]\n",
    "        expr_2 = [pl.min(col).alias(f\"min_depth2_{col}\") for col in cols]\n",
    "        expr_3 = [pl.mean(col).alias(f\"mean_depth2_{col}\") for col in cols] + \\\n",
    "            [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "        # expr_ngs = [pl.max(col).alias(f\"max_{col}\") for col in ['num_group1', 'num_group2', ]]\n",
    "\n",
    "        expr_4 = [pl.col(col).fill_null(strategy=\"zero\").map_elements(lambda x: x.max() - x.min(), return_dtype=pl.Float32).alias(f\"max-min_gap_depth2_{col}\") for col in ['collater_valueofguarantee_1124L', 'pmts_dpd_1073P', 'pmts_overdue_1140A',]]\n",
    "\n",
    "        expr_ngc = [pl.count(\"num_group2\").alias(f\"count_depth2_a2_num_group2\")]\n",
    "\n",
    "        # expr_5 = [pl.last(col).alias(f\"last_{col}\") for col in cols] + \\\n",
    "        #     [pl.first(col).alias(f\"first_{col}\") for col in cols] + \\\n",
    "        #     [pl.std(col).alias(f\"std_{col}\") for col in cols]\n",
    "\n",
    "        return expr_1 + expr_2 + expr_3 + expr_4 + expr_ngc # + expr_5\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df)\n",
    "\n",
    "        return exprs\n",
    "    \n",
    "    # no use from here\n",
    "    @staticmethod\n",
    "    def applprev2_exprs(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" not in col]\n",
    "        # expr_1 = [pl.max(col).alias(f\"max_{col}\") for col in cols] + [pl.min(col).alias(f\"min_{col}\") for col in cols] \n",
    "        expr_2 = [pl.first(col).alias(f\"first_{col}\") for col in cols]#  + [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        return []#expr_2\n",
    "    \n",
    "    @staticmethod\n",
    "    def bureau_b1(df):  # 0.95 filterにかかるため未使用\n",
    "        # cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        # expr_1 = [pl.max(col).alias(f\"bureau_b1_max_{col}\") for col in cols]\n",
    "        # expr_2 = [pl.min(col).alias(f\"bureau_b1_min_{col}\") for col in cols]\n",
    "\n",
    "        # return expr_1 + expr_2 #  + expr_3\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def bureau_b2(df):  # 0.95filterにかかるため未使用\n",
    "        # cols = [col for col in df.columns if (col[-1] in (\"T\",\"L\",\"M\",\"D\",\"P\",\"A\")) or (\"num_group\" in col)]\n",
    "\n",
    "        # expr_1 = [pl.max(col).alias(f\"bureau_b2_max_{col}\") for col in cols]\n",
    "        # expr_2 = [pl.min(col).alias(f\"bureau_b2_min_{col}\") for col in cols]\n",
    "\n",
    "        # return expr_1 + expr_2 #  + expr_3\n",
    "        return []\n",
    "\n",
    "    \n",
    "def agg_by_case(path, df):\n",
    "    path = str(path)\n",
    "    if '_applprev_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "\n",
    "#     elif '_applprev_2' in path:\n",
    "#         df = df.group_by(\"case_id\").agg(Aggregator.applprev2_exprs(df))\n",
    "\n",
    "    elif '_credit_bureau_a_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.bureau_a1(df))\n",
    "\n",
    "    elif '_credit_bureau_b_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.bureau_b1(df))\n",
    "\n",
    "    elif '_deposit_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.deposit_exprs(df))\n",
    "    elif '_debitcard_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.debitcard_exprs(df))\n",
    "        \n",
    "    elif '_tax_registry_a' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.tax_a_exprs(df))\n",
    "    elif '_tax_registry_b' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    elif '_tax_registry_c' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "    elif '_other_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.other_expr(df))\n",
    "    elif '_person_1' in path:\n",
    "        df = df.sort(\"num_group1\").group_by(\"case_id\").agg(Aggregator.person_expr(df))\n",
    "    elif '_person_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.person_2_expr(df))\n",
    "\n",
    "    elif '_credit_bureau_a_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.bureau_a2(df))\n",
    "    elif '_credit_bureau_b_2' in path:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_file(path, depth=None): \n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = agg_by_case(path, df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    print(regex_path)\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = agg_by_case(path, df)\n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base.with_columns(\n",
    "            decision_month = pl.col(\"date_decision\").dt.month(),\n",
    "            decision_weekday = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    print(df_data.info())\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "        # cat_cols = [c for c in cat_cols if 'diff_' not in c]\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618d030",
   "metadata": {
    "papermill": {
     "duration": 0.010184,
     "end_time": "2024-10-20T00:47:51.134086",
     "exception": false,
     "start_time": "2024-10-20T00:47:51.123902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b765a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:47:51.157288Z",
     "iopub.status.busy": "2024-10-20T00:47:51.156068Z",
     "iopub.status.idle": "2024-10-20T00:52:17.972492Z",
     "shell.execute_reply": "2024-10-20T00:52:17.971287Z"
    },
    "papermill": {
     "duration": 266.831037,
     "end_time": "2024-10-20T00:52:17.975464",
     "exception": false,
     "start_time": "2024-10-20T00:47:51.144427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/parquet_files/train/train_static_0_*.parquet\n",
      "../data/parquet_files/train/train_applprev_1_*.parquet\n",
      "../data/parquet_files/train/train_credit_bureau_a_1_*.parquet\n",
      "../data/parquet_files/train/train_credit_bureau_a_2_*.parquet\n"
     ]
    }
   ],
   "source": [
    "train_data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce6dc46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:52:17.999831Z",
     "iopub.status.busy": "2024-10-20T00:52:17.999102Z",
     "iopub.status.idle": "2024-10-20T00:52:37.202319Z",
     "shell.execute_reply": "2024-10-20T00:52:37.201071Z"
    },
    "papermill": {
     "duration": 19.218331,
     "end_time": "2024-10-20T00:52:37.205211",
     "exception": false,
     "start_time": "2024-10-20T00:52:17.986880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_overall = feature_eng(**train_data_store)\n",
    "y_train = train_overall[TARGET].to_pandas().astype(np.float32).values\n",
    "train = read_file(TRAIN_DIR / \"train_base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc44939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:52:37.228788Z",
     "iopub.status.busy": "2024-10-20T00:52:37.228376Z",
     "iopub.status.idle": "2024-10-20T00:52:37.904304Z",
     "shell.execute_reply": "2024-10-20T00:52:37.903079Z"
    },
    "papermill": {
     "duration": 0.690684,
     "end_time": "2024-10-20T00:52:37.906840",
     "exception": false,
     "start_time": "2024-10-20T00:52:37.216156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train_data_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bece3a8",
   "metadata": {
    "papermill": {
     "duration": 0.010351,
     "end_time": "2024-10-20T00:52:38.597688",
     "exception": false,
     "start_time": "2024-10-20T00:52:38.587337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# post fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d20c455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:52:38.659628Z",
     "iopub.status.busy": "2024-10-20T00:52:38.658337Z",
     "iopub.status.idle": "2024-10-20T00:53:47.825797Z",
     "shell.execute_reply": "2024-10-20T00:53:47.824319Z"
    },
    "papermill": {
     "duration": 69.18179,
     "end_time": "2024-10-20T00:53:47.828257",
     "exception": false,
     "start_time": "2024-10-20T00:52:38.646467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1526659 entries, 0 to 1526658\n",
      "Columns: 817 entries, case_id to last_empls_employer_name_740M\n",
      "dtypes: bool(1), float32(4), float64(660), int64(4), int8(2), object(146)\n",
      "memory usage: 9.2+ GB\n",
      "None\n",
      "Memory usage of dataframe is 8060.49 MB\n",
      "Memory usage after optimization is: 2986.55 MB\n",
      "Decreased by 62.9%\n"
     ]
    }
   ],
   "source": [
    "train_overall, str_features = to_pandas(train_overall)\n",
    "train_overall = reduce_mem_usage(train_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd76309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:48.518402Z",
     "iopub.status.busy": "2024-10-20T00:53:48.518036Z",
     "iopub.status.idle": "2024-10-20T00:53:57.192113Z",
     "shell.execute_reply": "2024-10-20T00:53:57.190952Z"
    },
    "papermill": {
     "duration": 8.690737,
     "end_time": "2024-10-20T00:53:57.196191",
     "exception": false,
     "start_time": "2024-10-20T00:53:48.505454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = train_overall.pipe(Pipeline.filter_cols)\n",
    "train_overall = train_overall.drop(drop_cols, axis=1)\n",
    "#test_overall = test_overall.drop(str_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5939af98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:57.223275Z",
     "iopub.status.busy": "2024-10-20T00:53:57.222604Z",
     "iopub.status.idle": "2024-10-20T00:53:57.231420Z",
     "shell.execute_reply": "2024-10-20T00:53:57.230340Z"
    },
    "papermill": {
     "duration": 0.025386,
     "end_time": "2024-10-20T00:53:57.234657",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.209271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n"
     ]
    }
   ],
   "source": [
    "selected_features = sorted([i for i in train_overall.columns if i not in [\"case_id\", \"MONTH\", \"WEEK_NUM\", \"target\"]])\n",
    "cat_features_idx = [i for i, col in enumerate(selected_features) if col in str_features]\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31828a1",
   "metadata": {
    "papermill": {
     "duration": 0.011202,
     "end_time": "2024-10-20T00:53:57.257568",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.246366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# treemodel config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369ab07a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:57.282746Z",
     "iopub.status.busy": "2024-10-20T00:53:57.282365Z",
     "iopub.status.idle": "2024-10-20T00:53:57.290712Z",
     "shell.execute_reply": "2024-10-20T00:53:57.289565Z"
    },
    "papermill": {
     "duration": 0.023639,
     "end_time": "2024-10-20T00:53:57.293051",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.269412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {    \n",
    "    \"lgb\": {\n",
    "        \"num_round\": 10000,\n",
    "        \"es_round\" : 20,\n",
    "        \"verbose_eval\": 100,\n",
    "        \"params\" : {\n",
    "            'objective': 'binary', \n",
    "            \"metric\": \"auc\",\n",
    "            'learning_rate': 0.02,\n",
    "            'scale_pos_weight': 10,\n",
    "            'boosting': 'gbdt',\n",
    "            'verbose': -1,\n",
    "            'seed': 42,\n",
    "            'num_leaves': 64, \n",
    "            \"reg_alpha\": 0.1,\n",
    "            \"reg_lambda\": 10,\n",
    "            \"cat_smooth\": 20,\n",
    "        },   \n",
    "    },\n",
    "    \"cb\": {\n",
    "        \"task_type\": \"classification\",\n",
    "        \"es_round\" : 20,\n",
    "        \"verbose_eval\": 500,\n",
    "        \"params\" : { \n",
    "            'random_seed': 42,\n",
    "            \"learning_rate\": 0.04,\n",
    "            'use_best_model': True,\n",
    "            'iterations': 10000,\n",
    "            'reg_lambda': 10,\n",
    "            \"scale_pos_weight\": 10,\n",
    "            \"task_type\": \"GPU\",\n",
    "            'loss_function': 'Logloss',\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52538ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:57.318802Z",
     "iopub.status.busy": "2024-10-20T00:53:57.317979Z",
     "iopub.status.idle": "2024-10-20T00:53:57.322990Z",
     "shell.execute_reply": "2024-10-20T00:53:57.321871Z"
    },
    "papermill": {
     "duration": 0.02042,
     "end_time": "2024-10-20T00:53:57.325109",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.304689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "treemodel = TreeModelling(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73642298",
   "metadata": {
    "papermill": {
     "duration": 0.011372,
     "end_time": "2024-10-20T00:53:57.348032",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.336660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e540b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:57.373087Z",
     "iopub.status.busy": "2024-10-20T00:53:57.372711Z",
     "iopub.status.idle": "2024-10-20T00:53:57.444199Z",
     "shell.execute_reply": "2024-10-20T00:53:57.443295Z"
    },
    "papermill": {
     "duration": 0.087101,
     "end_time": "2024-10-20T00:53:57.446852",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.359751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_train_week_nums = train.unique(\"WEEK_NUM\").sort(\"WEEK_NUM\").select(\"WEEK_NUM\").to_numpy().reshape(-1)\n",
    "train_week_df = train.select(\"WEEK_NUM\").to_pandas()\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "tmp = train_overall[[\"WEEK_NUM\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6626d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T00:53:57.472981Z",
     "iopub.status.busy": "2024-10-20T00:53:57.471492Z",
     "iopub.status.idle": "2024-10-20T03:23:32.302804Z",
     "shell.execute_reply": "2024-10-20T03:23:32.299967Z"
    },
    "papermill": {
     "duration": 8974.850751,
     "end_time": "2024-10-20T03:23:32.309177",
     "exception": false,
     "start_time": "2024-10-20T00:53:57.458426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n"
     ]
    }
   ],
   "source": [
    "lgb_models = []\n",
    "lgb_output =np.zeros(len(train_overall))\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(train_overall, y_train, groups=train_week_df)):\n",
    "    print(\"Fold :\", fold)\n",
    "    train_week = train_week_df.loc[tr_idx].values\n",
    "    valid_week = train_week_df.loc[val_idx].values\n",
    "    lgb_model, lgb_val_output = treemodel.lgb_train_and_valid(train_overall.loc[tr_idx][selected_features], y_train[tr_idx],\n",
    "                                                              train_overall.loc[val_idx][selected_features], y_train[val_idx],\n",
    "                                                              config[\"lgb\"][\"params\"])\n",
    "    lgb_models.append(lgb_model)\n",
    "    lgb_output[val_idx] = lgb_val_output\n",
    "    print(\"----\")\n",
    "    \n",
    "tmp[\"score\"] = lgb_output\n",
    "print(roc_auc_score(tmp[\"target\"], tmp[\"score\"]))\n",
    "\n",
    "if not EVAL:\n",
    "    lgb_test_predictions = treemodel.lgb_test_by_batch(lgb_models, test_overall, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640377b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T03:23:32.353545Z",
     "iopub.status.busy": "2024-10-20T03:23:32.353035Z",
     "iopub.status.idle": "2024-10-20T03:23:32.369323Z",
     "shell.execute_reply": "2024-10-20T03:23:32.368292Z"
    },
    "papermill": {
     "duration": 0.042389,
     "end_time": "2024-10-20T03:23:32.371973",
     "exception": false,
     "start_time": "2024-10-20T03:23:32.329584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not EVAL:\n",
    "    \n",
    "    cb_models = []\n",
    "    cb_output =np.zeros(len(train_cb))\n",
    "    for fold, (tr_idx, val_idx) in enumerate(cv.split(train_cb, y_train, groups=train_week_df)):\n",
    "        print(\"Fold :\", fold)\n",
    "        cb_model, cb_val_output = treemodel.cb_train_and_valid(\n",
    "                                                            train_overall.loc[tr_idx][selected_features], y_train[tr_idx],\n",
    "                                                            train_overall.loc[val_idx][selected_features], y_train[val_idx],\n",
    "                                                            config[\"cb\"][\"params\"])\n",
    "        cb_models.append(cb_model)\n",
    "        cb_output[val_idx] = cb_val_output\n",
    "        print(\"----\")\n",
    "     \n",
    "    tmp[\"score\"] = cb_output\n",
    "    print(roc_auc_score(tmp[\"target\"], tmp[\"score\"]))\n",
    "    tmp[\"score\"] = (cb_output + lgb_output) / 2\n",
    "    print(roc_auc_score(tmp[\"target\"], tmp[\"score\"]))\n",
    "\n",
    "    cb_test_predictions = treemodel.cb_numpy_test_by_batch(cb_models, test_cb, batch_size = 10000)\n",
    "\n",
    "    del train_cb, test_cb\n",
    "    \n",
    "    test_predictions = (lgb_test_predictions + cb_test_predictions) / 2\n",
    "    # submission\n",
    "    submission = pd.DataFrame({\n",
    "            \"case_id\": test_case_ids,\n",
    "            \"score\": test_predictions\n",
    "        }).set_index('case_id')\n",
    "    submission.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60267d65",
   "metadata": {
    "papermill": {
     "duration": 0.018596,
     "end_time": "2024-10-20T03:23:32.408901",
     "exception": false,
     "start_time": "2024-10-20T03:23:32.390305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "datasetId": 4424545,
     "sourceId": 7600559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9352.127713,
   "end_time": "2024-10-20T03:23:35.411978",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-20T00:47:43.284265",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
